# Developer Quick Start Guide

> **Forecasting Model Developer Handoff Package**  
> **Version:** 1.0.0

---

## ğŸš€ Quick Start

```python
import pandas as pd

# Load the main forecasting dataset
features = pd.read_parquet('data/features_place_item_week.parquet')

# Split by train/val/test
train = features[features['train_val_test_flag'] == 'train']
val = features[features['train_val_test_flag'] == 'val']
test = features[features['train_val_test_flag'] == 'test']

# Get demand classification
classification = pd.read_csv('data/demand_classification.csv')

print(f"Train: {len(train):,} rows")
print(f"Val: {len(val):,} rows")
print(f"Test: {len(test):,} rows")
```

---

## ğŸ“ Folder Structure

```
new_developer_items/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ orders_clean.parquet         # Cleaned orders
â”‚   â”œâ”€â”€ order_items_clean.parquet    # Cleaned line items
â”‚   â”œâ”€â”€ dim_items_clean.parquet      # Product dimension
â”‚   â”œâ”€â”€ dim_places_clean.parquet     # Location dimension
â”‚   â”œâ”€â”€ weekly_place_item.parquet    # Weekly aggregates
â”‚   â”œâ”€â”€ features_place_item_week.parquet  # â­ MAIN DATASET
â”‚   â””â”€â”€ demand_classification.csv    # SBC classification
â”œâ”€â”€ schema/
â”‚   â”œâ”€â”€ orders_schema.json
â”‚   â”œâ”€â”€ order_items_schema.json
â”‚   â”œâ”€â”€ weekly_place_item_schema.json
â”‚   â””â”€â”€ features_schema.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_cleaning.py
â”‚   â”œâ”€â”€ 02_aggregation.py
â”‚   â”œâ”€â”€ 03_feature_engineering.py
â”‚   â””â”€â”€ validate_checks.py
â”œâ”€â”€ manifest.json
â”œâ”€â”€ DATA_README.md
â””â”€â”€ README_FOR_DEVELOPER.md          # â¬…ï¸ You are here
```

---

## ğŸ¯ Primary Forecast Dataset

**File:** `data/features_place_item_week.parquet`

| Column | Type | Description |
|--------|------|-------------|
| `place_id` | int | Location ID |
| `item_id` | int | Product ID |
| `week_start` | datetime | Week start (Monday, UTC) |
| `demand` | int | **TARGET** - Weekly quantity |
| `lag_1w` ... `lag_52w` | float | Lag features |
| `roll_mean_4w` ... | float | Rolling statistics |
| `demand_type` | str | SBC classification |
| `train_val_test_flag` | str | Split assignment |

---

## âš ï¸ Critical Rules

### 1. NEVER Use Future Data
All features use **past data only**. Lag features are shifted, rolling stats exclude current period.

### 2. Respect Train/Val/Test Split
```python
# âŒ DON'T: Train on all data
model.fit(features)

# âœ… DO: Use only train split
train = features[features['train_val_test_flag'] == 'train']
model.fit(train)
```

### 3. Handle Demand Types Differently

```python
# Route to appropriate model
smooth = features[features['demand_type'] == 'Smooth']      # â†’ ETS/Prophet
intermittent = features[features['demand_type'] == 'Intermittent']  # â†’ Croston
lumpy = features[features['demand_type'] == 'Lumpy']        # â†’ ML ensemble
```

---

## ğŸ“Š Model Selection by Demand Type

| Type | % Items | Recommended | Alternative |
|------|---------|-------------|-------------|
| **Smooth** | 4% | ETS, Prophet | LightGBM |
| **Erratic** | 2% | LightGBM | XGBoost |
| **Intermittent** | 47% | Croston | SBA |
| **Lumpy** | 17% | ISBTS | ML ensemble |
| **Insufficient** | 30% | Safety stock rules | â€” |

---

## ğŸ“ˆ Evaluation Metrics

**Primary:** WAPE (Weighted Absolute Percentage Error)

```python
def wape(y_true, y_pred):
    return np.abs(y_true - y_pred).sum() / y_true.sum()
```

**Also track:**
- Bias: `(y_pred.sum() - y_true.sum()) / y_true.sum()`
- RMSE: `np.sqrt(((y_true - y_pred) ** 2).mean())`

---

## ğŸ”§ Regenerating Data

To regenerate the dataset from raw CSVs:

```bash
cd new_developer_items/scripts
python 01_cleaning.py           # ~2 min
python 02_aggregation.py        # ~1 min
python 03_feature_engineering.py # ~3 min
python validate_checks.py       # Validates output
```

---

## ğŸ“š Additional Documentation

| Document | Contents |
|----------|----------|
| `DATA_README.md` | Full data specification |
| `manifest.json` | Row counts, versions, metadata |
| `schema/*.json` | JSON schemas for validation |
| `../readme2.md` | Complete analytical report |

---

## âœ… Pre-Flight Checklist

Before model training:

- [ ] Loaded `features_place_item_week.parquet`
- [ ] Filtered to `train_val_test_flag == 'train'`
- [ ] Filtered by `demand_type` for model routing
- [ ] Verified no future leakage (run `validate_checks.py`)
- [ ] Selected appropriate model for demand type
- [ ] Set up WAPE as primary metric

---

## ğŸ†˜ Common Issues

### Memory Errors
The features file is ~150MB. If memory is tight:
```python
# Filter to specific demand type
df = pd.read_parquet('data/features_place_item_week.parquet',
                     filters=[('demand_type', '==', 'Smooth')])
```

### Missing Lag Values
First 52 weeks have NaN for `lag_52w`. These are filled with 0:
```python
# Already filled, but verify
assert features['lag_52w'].isna().sum() == 0
```

---

**Good luck with your forecasting model! ğŸ¯**
